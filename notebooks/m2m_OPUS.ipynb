{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab83c4d-cd3d-4763-870b-d436e6339bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"nmt_training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd983fe-888e-48c9-986b-8529b1131a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating every 30 training steps.\n"
     ]
    }
   ],
   "source": [
    "import datasets \n",
    "import transformers\n",
    "\n",
    "from nmt_clean.config import config\n",
    "from nmt_clean.config_m2m_mul_en import config as configm2m\n",
    "from nmt_clean.load_data import load_training_data, load_validation_data, load_testing_data\n",
    "from nmt_clean.metrics import compute_sacreBLEU as compute_metrics\n",
    "from nmt_clean.preprocess import Many2ManyProcessor\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(config[\"model_checkpoint\"])\n",
    "main_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(config['model_checkpoint'])\n",
    "data_collator = transformers.DataCollatorForSeq2Seq(tokenizer, model = main_model) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80e8a8d8-f1d1-4278-93ae-5760a2dea2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute metrics eval_lang to eval_pairs\n",
    "#set the tgt tokens in the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c00367c1-0610-4ade-9549-ede50e87638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#tokenizer.lang_code_to_id = {}\n",
    "#largest_token = len(tokenizer)\n",
    "#for i, code in enumerate([\"ach\", \"lgg\", \"lug\", \"nyn\", \"teo\", \"en\"]):\n",
    "#    tokenizer.lang_code_to_id[code] = i + largest_token\n",
    "    #tokenizer.fairseq_ids_to_tokens[i + largest_token] = code\n",
    "\n",
    "metric = datasets.load_metric('sacrebleu')\n",
    "processor = Many2ManyProcessor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3654798-c8a2-4974-984d-c936157ab633",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#paired_datasets = processor.dataset_from_folders_m2m(configm2m[\"training_subset_paths\"])\n",
    "#tokenizer = AutoTokenizer.from_pretrained(config[\"model_checkpoint\"])\n",
    "\n",
    "#paired_datasets = datasets.concatenate_datasets(paired_datasets)\n",
    "#paired_datasets = paired_datasets.shuffle(seed=42)\n",
    "#paired_datasets = paired_datasets.flatten_indices()  # rewrite the shuffled dataset on disk as contiguous chunks of data\n",
    "\n",
    "#training_tokens = processor.iterative_preprocess([paired_datasets],tokenizer )\n",
    "\n",
    "\n",
    "\n",
    "#validation_cutoff = 50\n",
    "\n",
    "#os.environ[\"WANDB_API_KEY\"] = secret_value_0\n",
    "#wandb.init(project=config['wandb_project'],entity=config[\"wandb_entity\"], config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7aab11-345b-4102-a978-af262419a4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 13.03it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_datasets = processor.dataset_from_folders_m2m(configm2m[\"validation_subset_paths\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e74dc207-30a8-421a-a558-621f6554b9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123780"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4126*30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a138d0da-dedc-4f09-806d-c1a89a35e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_preds = []\n",
    "for i in range(len(valid_datasets)):\n",
    "    eval_preds.append(valid_datasets[i].src_language + \"_\" + valid_datasets[i].tgt_language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "776746f6-36ce-4c87-9e70-8f34e7dff59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink_validation(dataset_to_shrink, eval_pairs, subset_len = 4126, samples_per_language = 100, pair_needed = \"en\"):\n",
    "    subsets = []\n",
    "    shrunk_eval_pairs = [] \n",
    "    assert subset_len > samples_per_language\n",
    "\n",
    "    for language_pair, beginning_idx in zip(eval_pairs, range(0,len(dataset_to_shrink), subset_len)):\n",
    "        if pair_needed not in language_pair:\n",
    "            continue\n",
    "        subset = datasets.Dataset.from_dict(dataset_to_shrink[beginning_idx:beginning_idx+samples_per_language])\n",
    "        subsets.append(subset)\n",
    "        shrunk_eval_pairs.append(language_pair)\n",
    "    return subsets, shrunk_eval_pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "739a21ae-3476-4d35-b277-695a16c87e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration mekaneeky--salt_m2m_ready-9935f7282ac022da\n",
      "WARNING:datasets.builder:Found cached dataset parquet (/home/jupyter/.cache/huggingface/datasets/mekaneeky___parquet/mekaneeky--salt_m2m_ready-9935f7282ac022da/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f091d16b644a2e9f11fd3a0832d9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_dataset = datasets.load_dataset(\"mekaneeky/salt_m2m_ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "333b818d-0d89-4210-8f18-b7b5f07d1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shrunk_validation, shrunk_eval_preds = shrink_validation(processed_dataset[\"validation\"], eval_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5c19078-7087-4df7-9cec-6913cf4bb9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['train_settings'].max_steps = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a303be-2df1-4e75-b1ff-7672559490b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using cuda_amp half precision backend\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3683940\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2400\n",
      "  Gradient Accumulation steps = 2400\n",
      "  Total optimization steps = 300\n",
      "  Number of trainable parameters = 76994560\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mazawahry\u001b[0m (\u001b[33msunbird\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/nmt_training/wandb/run-20230302_151913-2spicvzz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sunbird/huggingface/runs/2spicvzz' target=\"_blank\">salt-en</a></strong> to <a href='https://wandb.ai/sunbird/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sunbird/huggingface' target=\"_blank\">https://wandb.ai/sunbird/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sunbird/huggingface/runs/2spicvzz' target=\"_blank\">https://wandb.ai/sunbird/huggingface/runs/2spicvzz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main_model.config.use_cache = False\n",
    "\n",
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    main_model,\n",
    "    config['train_settings'],\n",
    "    train_dataset = processed_dataset[\"train\"],\n",
    "    eval_dataset = shrunk_validation,\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = lambda x: compute_metrics(\n",
    "        x, eval_preds , config['validation_samples_per_language'], tokenizer,metric ),\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 5)],\n",
    ")\n",
    "#trainer.config = config['train_settings']\n",
    "#trainer.config.max_length = config[\"max_input_length\"] #FIXME issue PR to the repo\n",
    "#trainer.config.num_beams = 5 #FIXME issue PR to the repo\n",
    "\n",
    "train_result = trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd5303-4386-4854-ac24-0ee8276a0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65095c89-f6a5-4901-a8cb-48a2975546ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
