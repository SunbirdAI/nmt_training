{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython import display\n!pip install transformers\n!pip install sacrebleu\n!pip install sacremoses\n!pip install datasets\n!pip install wandb\n!pip install sentencepiece\n!pip install numpy requests nlpaug\n!wget https://raw.githubusercontent.com/SunbirdAI/nmt_training/main/salt_v2/salt.py\n!wget https://raw.githubusercontent.com/SunbirdAI/nmt_training/main/nmt_clean/augmentations.py\ndisplay.clear_output()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-15T23:49:58.581407Z","iopub.execute_input":"2023-03-15T23:49:58.582126Z","iopub.status.idle":"2023-03-15T23:51:10.486191Z","shell.execute_reply.started":"2023-03-15T23:49:58.582087Z","shell.execute_reply":"2023-03-15T23:51:10.484765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from augmentations import Augmentations\nimport datasets\nfrom IPython import display\nimport nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.word as naw\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\nimport sentencepiece\nimport sacrebleu\nimport sacremoses\nimport salt\nimport tqdm\nimport transformers\nimport torch\nimport wandb\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T23:55:03.243029Z","iopub.execute_input":"2023-03-15T23:55:03.244058Z","iopub.status.idle":"2023-03-15T23:55:03.251303Z","shell.execute_reply.started":"2023-03-15T23:55:03.244015Z","shell.execute_reply":"2023-03-15T23:55:03.250090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2023-03-15T23:55:03.835795Z","iopub.execute_input":"2023-03-15T23:55:03.836880Z","iopub.status.idle":"2023-03-15T23:55:03.845134Z","shell.execute_reply.started":"2023-03-15T23:55:03.836839Z","shell.execute_reply":"2023-03-15T23:55:03.843829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Parameters for mul-en models\nconfig = {\n    'source_language': 'many',\n    'target_language': 'eng',\n    'metric_for_best_model': 'loss',\n    'effective_train_batch_size': 5000,\n    'max_input_length': 128,\n    'validation_samples_per_language': 500,\n    'eval_languages': [\"ach\", \"lgg\", \"lug\", \"nyn\", \"teo\", \"luo\"],\n    'eval_pretrained_model': True,\n    'learning_rate': 5e-4,\n    'num_train_epochs': 10,\n    'label_smoothing_factor': 0.1,\n    'mt560_relative_sample_rate' : 0.2,\n    'flores200_training_data': True,\n    'mt560_training_data': True,\n    'monolingual_training_data': False,\n    'back_translation_training_data': True,\n    'google_back_translation_data': True,\n    'named_entities_training_data': False,\n    'lafand_training_data': True,\n    'tag_subsets': True,\n    'early_stopping_patience': 4,\n    'eval_steps_interval': 100,\n    'data_dir': 'salt-translation-plus-external-datasets-15-3-23',\n}\n\nconfig['language_pair'] = (f'{config[\"source_language\"]}-'\n                           f'{config[\"target_language\"]}')\nconfig['wandb_project'] = f'sunbird-translate-{config[\"language_pair\"]}'\nconfig['model_checkpoint'] = f'Helsinki-NLP/opus-mt-mul-en'\n#config['model_checkpoint'] = (\n#    '/content/gdrive/MyDrive/Translation/saved_models/'\n#    'marianmt-many-eng/checkpoint-1400')\n\n\n# Find the biggest batch size that fits in GPU memory\nAPPROX_MODEL_MEMORY_SIZE_MB = 310\nif torch.cuda.is_available():\n  gpu_info = !nvidia-smi\n  gpu_memory_mb = int(gpu_info[9].split()[10][:-3])\n  per_device_max_batch_size = int(gpu_memory_mb / APPROX_MODEL_MEMORY_SIZE_MB)\n  B = config['effective_train_batch_size'] \n  factors = np.array([x for x in range(1, B) if B % x == 0])\n  config['train_batch_size'] = int(max(\n      factors[factors < per_device_max_batch_size]))\n  config['eval_batch_size'] = config['train_batch_size']\nelse:\n  config['train_batch_size'] = 1\n  config['eval_batch_size'] = 1\nconfig['gradient_accumulation_steps'] = int(\n    config['effective_train_batch_size'] / config['train_batch_size'])\n\n# Trainer settings\nconfig['train_settings'] = transformers.Seq2SeqTrainingArguments(\n    output_dir= f'/kaggle/working/best',\n    evaluation_strategy = 'steps',\n    eval_steps = config['eval_steps_interval'],\n    save_steps = config['eval_steps_interval'],\n    gradient_accumulation_steps = config['gradient_accumulation_steps'],\n    learning_rate = config['learning_rate'],\n    per_device_train_batch_size = config['train_batch_size'],\n    per_device_eval_batch_size = config['eval_batch_size'],\n    weight_decay = 0.01,\n    save_total_limit = 3,\n    num_train_epochs = config['num_train_epochs'],\n    predict_with_generate = True,\n    fp16 = torch.cuda.is_available(),\n    logging_dir = f'/kaggle/working/best',\n    report_to = 'wandb',\n    run_name = f'{config[\"source_language\"]}-{config[\"target_language\"]}',\n    load_best_model_at_end=True,\n    metric_for_best_model = config['metric_for_best_model'],\n    label_smoothing_factor = config['label_smoothing_factor']\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-16T00:06:33.537602Z","iopub.execute_input":"2023-03-16T00:06:33.537976Z","iopub.status.idle":"2023-03-16T00:06:33.669555Z","shell.execute_reply.started":"2023-03-16T00:06:33.537941Z","shell.execute_reply":"2023-03-16T00:06:33.668097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config['training_subset_ids'] = ['salt-train', 'ai4d']\n\nif config['lafand_training_data']:\n    config['training_subset_ids'].extend(['lafand-en-lug-combined', 'lafand-en-luo-combined'])\n\nif config['flores200_training_data']:\n    config['training_subset_ids'] .append('flores200')\n\nif config['back_translation_training_data']:\n    config['training_subset_ids'].extend( ['bt_ach_en_14_3_23', 'bt_lug_en_14_3_23'])\n\nif config['back_translation_training_data']:\n    config['training_subset_ids'].extend(['backtranslated-from-eng-google', 'backtranslated-from-lug-google' ])\n\nconfig['training_subset_ids'] = config['training_subset_ids']*5\n\nif config['mt560_training_data']:\n    config['training_subset_ids'].extend([\n        'mt560_ach', 'mt560_lug', 'mt560_nyn','mt560_luo'])\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-16T00:01:36.636022Z","iopub.execute_input":"2023-03-16T00:01:36.636388Z","iopub.status.idle":"2023-03-16T00:01:36.643993Z","shell.execute_reply.started":"2023-03-16T00:01:36.636355Z","shell.execute_reply":"2023-03-16T00:01:36.642776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.exists('salt-translation-plus-external-datasets-15-3-23'):\n    !wget https://sunbird-translate.s3.us-east-2.amazonaws.com/salt-translation-plus-external-datasets-15-3-23.zip\n    !unzip salt-translation-plus-external-datasets-15-3-23.zip\n    display.clear_output()","metadata":{"execution":{"iopub.status.busy":"2023-03-15T23:55:06.264809Z","iopub.execute_input":"2023-03-15T23:55:06.265203Z","iopub.status.idle":"2023-03-15T23:55:06.272118Z","shell.execute_reply.started":"2023-03-15T23:55:06.265169Z","shell.execute_reply":"2023-03-15T23:55:06.271013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\ntraining_subsets = [\n    salt.translation_dataset(\n        path=f'{config[\"data_dir\"]}/{id}.jsonl',\n        source_language=config['source_language'],\n        target_language=config['target_language'],\n        allow_target_language_in_source=False,\n        prefix_target_language_in_source=False,\n        languages_to_include=config['eval_languages'],\n        keep_unaugmented_src = False)\n    for id in tqdm(config['training_subset_ids'])\n]","metadata":{"execution":{"iopub.status.busy":"2023-03-16T00:01:39.712100Z","iopub.execute_input":"2023-03-16T00:01:39.712480Z","iopub.status.idle":"2023-03-16T00:01:54.324677Z","shell.execute_reply.started":"2023-03-16T00:01:39.712445Z","shell.execute_reply":"2023-03-16T00:01:54.323613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_probabilities = np.array([len(s) for s in training_subsets])\n# sample_probabilities = sample_probabilities * np.array(\n#     [config['mt560_relative_sample_rate'] if ('mt560' in id) or ('google' in id)  else 1.0\n#      for id in config['training_subset_ids']])\n# sample_probabilities = sample_probabilities / np.sum(sample_probabilities)\n\n# train_data_raw = datasets.interleave_datasets(\n#     training_subsets, sample_probabilities)\ntrain_data_raw = datasets.concatenate_datasets(training_subsets)\ntrain_data_raw = train_data_raw.shuffle()\ntrain_data_raw = train_data_raw.flatten_indices()","metadata":{"execution":{"iopub.status.busy":"2023-03-16T00:04:43.691313Z","iopub.execute_input":"2023-03-16T00:04:43.691688Z","iopub.status.idle":"2023-03-16T00:05:11.680569Z","shell.execute_reply.started":"2023-03-16T00:04:43.691656Z","shell.execute_reply":"2023-03-16T00:05:11.679597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_subsets = [\n    salt.translation_dataset(\n        path=f'{config[\"data_dir\"]}/salt-dev.jsonl',\n        source_language=language,\n        target_language=\"eng\",\n        keep_unaugmented_src = False,\n        allow_target_language_in_source=False,\n        prefix_target_language_in_source=False\n    )\n    for language in config['eval_languages']\n]\nvalidation_data_raw = datasets.concatenate_datasets(validation_subsets)","metadata":{"execution":{"iopub.status.busy":"2023-03-16T00:05:41.454164Z","iopub.execute_input":"2023-03-16T00:05:41.454883Z","iopub.status.idle":"2023-03-16T00:05:41.501385Z","shell.execute_reply.started":"2023-03-16T00:05:41.454845Z","shell.execute_reply":"2023-03-16T00:05:41.500296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentence_format(input):\n    '''Ensure capital letter at the start and full stop at the end.'''\n    try:\n        input = input[0].capitalize() + input[1:]\n        if input[-1] not in ['.', '!', '?']:\n            input = input + '.'\n    except:\n        return \"\"\n    return input\n\ndef preprocess(examples):\n    normalizer = sacremoses.MosesPunctNormalizer()  \n    inputs = []\n    targets = []\n    for input, target in zip(examples['source'], examples['target']):\n        if not len(input):\n          input = target\n        inputs.append(sentence_format(normalizer.normalize(input)))\n        targets.append(sentence_format(normalizer.normalize(target)))\n    \n    model_inputs = tokenizer(\n        inputs, text_target=targets,\n        max_length=config['max_input_length'], truncation=True)\n\n    return model_inputs\n\ndef postprocess(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\n\ndef compute_metrics(eval_preds, eval_languages, samples_per_language):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n        \n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds, decoded_labels = postprocess(decoded_preds, decoded_labels)\n    result = {}\n    for i, lang in enumerate(eval_languages):\n        \n        result_subset = metric.compute(\n            predictions=decoded_preds[\n                i*samples_per_language:(i+1)*samples_per_language],\n            references=decoded_labels[\n                i*samples_per_language:(i+1)*samples_per_language])\n        result[f\"BLEU_{lang}\"] = result_subset[\"score\"]\n        \n    result[\"BLEU_mean\"] = np.mean(\n        [result[f\"BLEU_{lang}\"] for lang in eval_languages])\n    \n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-03-16T00:05:45.302065Z","iopub.execute_input":"2023-03-16T00:05:45.302441Z","iopub.status.idle":"2023-03-16T00:05:45.316655Z","shell.execute_reply.started":"2023-03-16T00:05:45.302410Z","shell.execute_reply":"2023-03-16T00:05:45.314817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = transformers.AutoModelForSeq2SeqLM.from_pretrained(config['model_checkpoint'])\ntokenizer = transformers.AutoTokenizer.from_pretrained(config['model_checkpoint'])\ndata_collator = transformers.DataCollatorForSeq2Seq(tokenizer, model = model) \nmetric = datasets.load_metric('sacrebleu')","metadata":{"execution":{"iopub.status.busy":"2023-03-16T00:05:46.551328Z","iopub.execute_input":"2023-03-16T00:05:46.551716Z","iopub.status.idle":"2023-03-16T00:06:17.492234Z","shell.execute_reply.started":"2023-03-16T00:05:46.551658Z","shell.execute_reply":"2023-03-16T00:06:17.491256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if config['target_language'] == 'many':\n    replacements = {'nyn': 'kin',\n                    'lgg': 'lin',\n                    'ach': 'tso',\n                    'teo': 'som',\n                    'luo': 'sna'}\n    for r in replacements:\n        if (f'>>{r}<<' not in tokenizer.encoder and\n            f'{replacements[r]}' in tokenizer.encoder):\n            tokenizer.encoder[f\">>{r}<<\"] = tokenizer.encoder[f\"{replacements[r]}\"]\n            del tokenizer.encoder[f\"{replacements[r]}\"]\n\n    # Check that all the evaluation language codes are mapped to something.\n    for r in config['eval_languages']:\n        if f'>>{r}<<' not in tokenizer.encoder:\n            raise ValueError(f'Language code {r} not found in the encoder.')","metadata":{"execution":{"iopub.status.busy":"2023-03-16T00:06:40.519958Z","iopub.execute_input":"2023-03-16T00:06:40.520517Z","iopub.status.idle":"2023-03-16T00:06:40.535138Z","shell.execute_reply.started":"2023-03-16T00:06:40.520478Z","shell.execute_reply":"2023-03-16T00:06:40.533495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data  = train_data_raw.map(\n    preprocess,\n    remove_columns=['source', 'target', 'source_language', 'target_language'],\n    batched=True)\n\nvalidation_data  = validation_data_raw.map(\n    preprocess,\n    remove_columns=['source', 'target', 'source_language', 'target_language'],\n    batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-16T00:06:42.913535Z","iopub.execute_input":"2023-03-16T00:06:42.913903Z","iopub.status.idle":"2023-03-16T00:07:01.962773Z","shell.execute_reply.started":"2023-03-16T00:06:42.913871Z","shell.execute_reply":"2023-03-16T00:07:01.961002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb_key\")\nimport os\nos.environ[\"WANDB_API_KEY\"] = secret_value_0\nwandb.init(project=config['wandb_project'], config=config, entity=\"azawahry\")\n\ntransformers.logging.set_verbosity_error()\n\ntrainer = transformers.Seq2SeqTrainer(\n    model,\n    config['train_settings'],\n    train_dataset = train_data,\n    eval_dataset = validation_data,\n    data_collator = data_collator,\n    tokenizer = tokenizer,\n    compute_metrics = lambda x: compute_metrics(\n        x, config['eval_languages'][:-1], config['validation_samples_per_language']),\n    callbacks = [\n        transformers.EarlyStoppingCallback(\n            early_stopping_patience = config['early_stopping_patience'])],\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T23:16:39.807648Z","iopub.status.idle":"2023-03-15T23:16:39.808416Z","shell.execute_reply.started":"2023-03-15T23:16:39.808162Z","shell.execute_reply":"2023-03-15T23:16:39.808189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if config['eval_pretrained_model']:\n    trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2023-03-15T23:16:39.809747Z","iopub.status.idle":"2023-03-15T23:16:39.810508Z","shell.execute_reply.started":"2023-03-15T23:16:39.810250Z","shell.execute_reply":"2023-03-15T23:16:39.810276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}