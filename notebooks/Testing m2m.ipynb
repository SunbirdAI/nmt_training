{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64788b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating every 20 training steps.\n"
     ]
    }
   ],
   "source": [
    "import datasets \n",
    "from nmt_clean.config_m2m_mul_en import config\n",
    "from nmt_clean.read_files import dataset_from_folders_m2m\n",
    "from nmt_clean.preprocess import Many2ManyProcessor\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d38189f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 6/6 [00:00<00:00, 27.24it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f2889ca1cf4ef79ac0c5081a56ed30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/.pyenv/versions/pytorch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 123780\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "\n",
    "preprocess = Many2ManyProcessor(tokenizer = tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "110becdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 6/6 [00:07<00:00,  1.20s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1211a9059d024782907c14c3e21b353c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/3684 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa9d76d22184d10b6363da068e1b4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3684 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty string\n",
      "empty string\n",
      "empty string\n",
      "empty string\n",
      "empty string\n"
     ]
    }
   ],
   "source": [
    "paired_datasets = preprocess.dataset_from_folders_m2m(config[\"training_subset_paths\"])\n",
    "\n",
    "paired_datasets = datasets.concatenate_datasets(paired_datasets)\n",
    "paired_datasets = paired_datasets.shuffle(seed=42)\n",
    "paired_datasets = paired_datasets.flatten_indices()  # rewrite the shuffled dataset on disk as contiguous chunks of data\n",
    "\n",
    "preprocessing_function = preprocess.attach_tokenizer(tokenizer)\n",
    "\n",
    "training_dataset = paired_datasets.map(preprocessing_function, remove_columns=[\"translation\"], batched=True)\n",
    "\n",
    "#training_tokens = preprocess.iterative_preprocess(paired_datasets,tokenizer )\n",
    "\n",
    "\n",
    "#tokens = preprocess.iterative_preprocess(paired_datasets,tokenizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0179073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [62,\n",
       "  233,\n",
       "  406,\n",
       "  246,\n",
       "  14,\n",
       "  67,\n",
       "  113,\n",
       "  14,\n",
       "  701,\n",
       "  6,\n",
       "  539,\n",
       "  907,\n",
       "  44,\n",
       "  184,\n",
       "  469,\n",
       "  11,\n",
       "  111,\n",
       "  61,\n",
       "  15,\n",
       "  6,\n",
       "  188,\n",
       "  256,\n",
       "  12,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [2110,\n",
       "  26969,\n",
       "  37,\n",
       "  300,\n",
       "  79,\n",
       "  186,\n",
       "  97,\n",
       "  475,\n",
       "  2553,\n",
       "  11909,\n",
       "  721,\n",
       "  245,\n",
       "  694,\n",
       "  28,\n",
       "  2943,\n",
       "  47,\n",
       "  579,\n",
       "  245,\n",
       "  32,\n",
       "  213,\n",
       "  292,\n",
       "  245,\n",
       "  150,\n",
       "  45724,\n",
       "  2337,\n",
       "  12,\n",
       "  0]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfe3b390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 6/6 [00:00<00:00, 21.76it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5bee7b6c2b423dbf95ba887f60888c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_paired_datasets = preprocess.dataset_from_folders_m2m(config[\"validation_subset_paths\"])\n",
    "\n",
    "val_paired_datasets = datasets.concatenate_datasets(val_paired_datasets)\n",
    "#val_paired_datasets = paired_datasets.shuffle(seed=42)\n",
    "#val_paired_datasets = paired_datasets.flatten_indices()  # rewrite the shuffled dataset on disk as contiguous chunks of data\n",
    "\n",
    "\n",
    "valid_dataset = val_paired_datasets.map(preprocessing_function, remove_columns=[\"translation\"], batched=True)\n",
    "\n",
    "#training_tokens = preprocess.iterative_preprocess(paired_datasets,tokenizer )\n",
    "\n",
    "\n",
    "#tokens = preprocess.iterative_preprocess(paired_datasets,tokenizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61b23760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3683940\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b157327e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 123780\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce70afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dict =  {\n",
    "            \"ach\":{\n",
    "                \"all\":[config['data_dir'] + \"v7.0/supervised/mul-en/test_ach.src\"],\n",
    "                \"en\":[],\n",
    "                \"lgg\":[],\n",
    "                \"lug\":[],\n",
    "                \"nyn\":[],\n",
    "                \"teo\":[]\n",
    "            },\n",
    "            \"en\": {\n",
    "                \"all\":[config['data_dir'] + \"v7.0/supervised/mul-en/test_ach.tgt\"], #all tgts here are the same file \n",
    "                \"ach\":[],\n",
    "                \"lgg\":[],\n",
    "                \"lug\":[],\n",
    "                \"nyn\":[],\n",
    "                \"teo\":[]\n",
    "            },\n",
    "            \"lgg\":{\n",
    "                \"all\":[config['data_dir'] + \"v7.0/supervised/mul-en/test_lgg.src\"],\n",
    "                \"en\":[],\n",
    "                \"ach\":[],\n",
    "                \"lug\":[],\n",
    "                \"nyn\":[],\n",
    "                \"teo\":[]\n",
    "            },\n",
    "            \"lug\":{\n",
    "                \"all\":[config['data_dir'] + \"v7.0/supervised/mul-en/test_lug.src\"],\n",
    "                \"en\":[],\n",
    "                \"lgg\":[],\n",
    "                \"ach\":[],\n",
    "                \"nyn\":[],\n",
    "                \"teo\":[]\n",
    "            },\n",
    "            \"nyn\":{\n",
    "                \"all\":[config['data_dir'] + \"v7.0/supervised/mul-en/test_nyn.src\"],\n",
    "                \"en\":[],\n",
    "                \"lgg\":[],\n",
    "                \"lug\":[],\n",
    "                \"ach\":[],\n",
    "                \"teo\":[]\n",
    "            },\n",
    "            \"teo\":{\n",
    "                \"all\":[config['data_dir'] + \"v7.0/supervised/mul-en/test_teo.src\"],\n",
    "                \"en\":[],\n",
    "                \"lgg\":[],\n",
    "                \"lug\":[],\n",
    "                \"nyn\":[],\n",
    "                \"ach\":[]\n",
    "            }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82c942ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 6/6 [00:00<00:00, 20.07it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903a5f9cba36473ba059940d2c09770a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_paired_datasets = preprocess.dataset_from_folders_m2m(test_data_dict)\n",
    "\n",
    "test_paired_datasets = datasets.concatenate_datasets(test_paired_datasets)\n",
    "#val_paired_datasets = paired_datasets.shuffle(seed=42)\n",
    "#val_paired_datasets = paired_datasets.flatten_indices()  # rewrite the shuffled dataset on disk as contiguous chunks of data\n",
    "\n",
    "\n",
    "test_dataset = test_paired_datasets.map(preprocessing_function, remove_columns=[\"translation\"], batched=True)\n",
    "\n",
    "#training_tokens = preprocess.iterative_preprocess(paired_datasets,tokenizer )\n",
    "\n",
    "\n",
    "#tokens = preprocess.iterative_preprocess(paired_datasets,tokenizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31844659",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dataset = datasets.DatasetDict({\n",
    "    \"train\": training_dataset, \n",
    "    \"validation\": valid_dataset,\n",
    "    \"test\":test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b2115c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8006c766bcac464197409a798e36ee13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9540f97f07f54b8ab8e5af2f41cd0110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1228 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a077d93825704de3bbf1ebeb2ed0933e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac039120ac5543fe8f3e84c5467202ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1228 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1068b350df36412880de8e2530656a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc21f65ca824fae8ce43d6812748be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1228 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4d4ba323eb48fa8b608da768bee9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split validation to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7b6af2d4e84d0089beb0f1585e300a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e3b4f118734cc3b76005e6076380e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/124 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548e643678374b4e93ecc549ba642e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split test to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb00b4e4d60d425ba87e03ac76c7b780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0ec033598643ccbf39de010b0c17b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/124 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932f81a600d147d7994e8efc89e51cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "whole_dataset.push_to_hub(\"mekaneeky/salt_m2m_ready\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53577f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _file_to_list(path):\n",
    "    with open(path) as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [line.rstrip() for line in lines]\n",
    "        return lines\n",
    "    \n",
    "def dataset_from_src_tgt_files(data_dir, dataset_id, read_first_n = 0):\n",
    "    path = os.path.join(data_dir, dataset_id)\n",
    "    source, target = [_file_to_list(path + '.src'),\n",
    "                      _file_to_list(path + '.tgt')]\n",
    "    if read_first_n:\n",
    "        source = source[:read_first_n]\n",
    "        target = target[:read_first_n]\n",
    "    pairs = {'translation': [{config['source_language']: s,\n",
    "                              config['target_language']: t}\n",
    "                             for s, t in zip(source, target)]}\n",
    "    return datasets.Dataset.from_dict(pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c88025d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_pair_dataset(cls, src_path,src_language, tgt_path, tgt_language, validation_cutoff =0, mode=\"cutoff_minimum\", marian_style_tokens = True):\n",
    "\n",
    "        src_scentences = cls.load_files(src_path)\n",
    "        tgt_scentences = cls.load_files(tgt_path)\n",
    "\n",
    "\n",
    "        if validation_cutoff:\n",
    "            if mode == \"cutoff_maximum\":\n",
    "                src_scentences = src_scentences[:validation_cutoff]\n",
    "                tgt_scentences = tgt_scentences[:validation_cutoff]\n",
    "            elif mode == \"cutoff_minimum\":\n",
    "                src_scentences = src_scentences[validation_cutoff:]\n",
    "                tgt_scentences = tgt_scentences[validation_cutoff:]\n",
    "\n",
    "        #TODO add tokens from tokenizer\n",
    "        if marian_style_tokens:\n",
    "            src_scentences = [tgt_language + \" \" + src for src in src_scentences]\n",
    "            #tgt_scentences = [language_token_dict[tgt_language] + \" \" + tgt for tgt in tgt_scentences]\n",
    "\n",
    "\n",
    "        pairs = {'translation': [{\"src\": s,\n",
    "                            \"tgt\": t}\n",
    "                            for s, t in zip(src_scentences, tgt_scentences)]\n",
    "        }\n",
    "        paired_dataset = datasets.Dataset.from_dict(pairs)\n",
    "        paired_dataset.src_language = src_language\n",
    "        paired_dataset.tgt_language = tgt_language\n",
    "        \n",
    "        return paired_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563d7e76",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m total_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtokens\u001b[49m)):\n\u001b[1;32m      3\u001b[0m     total_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(test[i])\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#print(f\"Src:{test[i].src_language} Tgt:{test[i].tgt_language} Count:{len(test[i])}\")\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "for i in range(len(tokens)):\n",
    "    total_count += len(test[i])\n",
    "    #print(f\"Src:{test[i].src_language} Tgt:{test[i].tgt_language} Count:{len(test[i])}\")\n",
    "    \n",
    "print(total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49a05778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['val_teo.src',\n",
       " 'test_ach.src',\n",
       " 'train_mt560_luo.src',\n",
       " 'val_nyn.src',\n",
       " 'val_teo.tgt',\n",
       " 'train_mt560_ach.tgt',\n",
       " 'val_lug.src',\n",
       " 'val_nyn.tgt',\n",
       " 'test_lgg.tgt',\n",
       " 'test_nyn.tgt',\n",
       " 'train_flores_lug.tgt',\n",
       " 'train_ai4d.tgt',\n",
       " 'val_lgg.src',\n",
       " 'test_lug.tgt',\n",
       " 'train.src',\n",
       " 'train_mt560_ach.src',\n",
       " 'val_ach.tgt',\n",
       " 'bukedde_ggl_bt_lug.tgt',\n",
       " 'test_teo.src',\n",
       " 'test_teo.tgt',\n",
       " 'train.tgt',\n",
       " 'train_ai4d.src',\n",
       " 'train_flores_luo.src',\n",
       " 'val_lgg.tgt',\n",
       " 'test_nyn.src',\n",
       " 'train_mt560_nyn.tgt',\n",
       " 'val_lug.tgt',\n",
       " 'train_mt560_luo.tgt',\n",
       " 'test_ach.tgt',\n",
       " 'test_lgg.src',\n",
       " 'train_mt560_lug.src',\n",
       " 'train_flores_lug.src',\n",
       " 'val_ach.src',\n",
       " 'test_lug.src',\n",
       " 'train_flores_luo.tgt',\n",
       " 'train_mt560_nyn.src',\n",
       " 'train_mt560_lug.tgt',\n",
       " 'bukedde_ggl_bt_lug.src']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad065d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e56a3f7d6087dc0e8010c68576613beaec95fa9bfb8de85e967e8c762a16959e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
