{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython import display\n!pip install transformers\n!pip install sacrebleu\n!pip install sacremoses\n!pip install datasets\n!pip install wandb\n!pip install sentencepiece\n!pip install numpy requests nlpaug\n!wget https://raw.githubusercontent.com/SunbirdAI/nmt_training/main/salt_v2/salt.py\n!wget https://raw.githubusercontent.com/SunbirdAI/nmt_training/main/nmt_clean/augmentations.py\ndisplay.clear_output()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-17T11:31:57.662802Z","iopub.execute_input":"2023-03-17T11:31:57.663241Z","iopub.status.idle":"2023-03-17T11:33:11.857032Z","shell.execute_reply.started":"2023-03-17T11:31:57.663201Z","shell.execute_reply":"2023-03-17T11:33:11.855769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from augmentations import Augmentations\nimport datasets\nfrom IPython import display\nimport nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.word as naw\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\nimport sentencepiece\nimport sacrebleu\nimport sacremoses\nimport salt\nimport tqdm\nimport transformers\nimport torch\nimport wandb\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T11:33:11.862118Z","iopub.execute_input":"2023-03-17T11:33:11.862803Z","iopub.status.idle":"2023-03-17T11:33:17.667931Z","shell.execute_reply.started":"2023-03-17T11:33:11.862760Z","shell.execute_reply":"2023-03-17T11:33:17.666779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2023-03-17T11:33:17.669894Z","iopub.execute_input":"2023-03-17T11:33:17.670270Z","iopub.status.idle":"2023-03-17T11:33:17.714973Z","shell.execute_reply.started":"2023-03-17T11:33:17.670229Z","shell.execute_reply":"2023-03-17T11:33:17.713941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Parameters for mul-en models\nconfig = {\n    'source_language': 'many',\n    'target_language': 'eng',\n    'metric_for_best_model': 'loss',\n    'effective_train_batch_size': 5000,\n    'max_input_length': 128,\n    'validation_samples_per_language': 500,\n    'eval_languages': [\"ach\", \"lgg\", \"lug\", \"nyn\", \"teo\", \"luo\"],\n    'eval_pretrained_model': True,\n    'learning_rate': 5e-5,\n    'num_train_epochs': 10,\n    'label_smoothing_factor': 0.1,\n    'mt560_relative_sample_rate' : 0.2,\n    'flores200_training_data': True,\n    'mt560_training_data': True,\n    'monolingual_training_data': False,\n    'back_translation_training_data': True,\n    'google_back_translation_data': True,\n    'named_entities_training_data': False,\n    'lafand_training_data': True,\n    'tag_subsets': True,\n    'early_stopping_patience': 4,\n    'eval_steps_interval': 50,\n    'data_dir': 'salt-translation-plus-external-datasets-15-3-23',\n}\n\nconfig['language_pair'] = (f'{config[\"source_language\"]}-'\n                           f'{config[\"target_language\"]}')\nconfig['wandb_project'] = f'sunbird-translate-{config[\"language_pair\"]}'\nconfig['model_checkpoint'] = f'/kaggle/input/nmt-marianmt-w-mafand/best/checkpoint-600'\n#config['model_checkpoint'] = (\n#    '/content/gdrive/MyDrive/Translation/saved_models/'\n#    'marianmt-many-eng/checkpoint-1400')\n\n\n# Find the biggest batch size that fits in GPU memory\nAPPROX_MODEL_MEMORY_SIZE_MB = 310\nif torch.cuda.is_available():\n  gpu_info = !nvidia-smi\n  gpu_memory_mb = int(gpu_info[9].split()[10][:-3])\n  per_device_max_batch_size = int(gpu_memory_mb / APPROX_MODEL_MEMORY_SIZE_MB)\n  B = config['effective_train_batch_size'] \n  factors = np.array([x for x in range(1, B) if B % x == 0])\n  config['train_batch_size'] = int(max(\n      factors[factors < per_device_max_batch_size]))\n  config['eval_batch_size'] = config['train_batch_size']\nelse:\n  config['train_batch_size'] = 1\n  config['eval_batch_size'] = 1\nconfig['gradient_accumulation_steps'] = int(\n    config['effective_train_batch_size'] / config['train_batch_size'])\n\n# Trainer settings\nconfig['train_settings'] = transformers.Seq2SeqTrainingArguments(\n    output_dir= f'/kaggle/working/best',\n    evaluation_strategy = 'steps',\n    eval_steps = config['eval_steps_interval'],\n    save_steps = config['eval_steps_interval'],\n    gradient_accumulation_steps = config['gradient_accumulation_steps'],\n    learning_rate = config['learning_rate'],\n    per_device_train_batch_size = config['train_batch_size'],\n    per_device_eval_batch_size = config['eval_batch_size'],\n    weight_decay = 0.01,\n    save_total_limit = 3,\n    num_train_epochs = config['num_train_epochs'],\n    predict_with_generate = True,\n    fp16 = torch.cuda.is_available(),\n    logging_dir = f'/kaggle/working/best',\n    report_to = 'wandb',\n    run_name = f'{config[\"source_language\"]}-{config[\"target_language\"]}',\n    load_best_model_at_end=True,\n    metric_for_best_model = config['metric_for_best_model'],\n    label_smoothing_factor = config['label_smoothing_factor']\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T11:51:54.393017Z","iopub.execute_input":"2023-03-17T11:51:54.393382Z","iopub.status.idle":"2023-03-17T11:51:54.570685Z","shell.execute_reply.started":"2023-03-17T11:51:54.393348Z","shell.execute_reply":"2023-03-17T11:51:54.567042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config['training_subset_ids'] = ['salt-train', 'ai4d']\n\nif config['lafand_training_data']:\n    config['training_subset_ids'].extend(['lafand-en-lug-combined', 'lafand-en-luo-combined'])\n\nif config['flores200_training_data']:\n    config['training_subset_ids'] .append('flores200')\n\nif config['back_translation_training_data']:\n    config['training_subset_ids'].extend( ['bt_ach_en_14_3_23', 'bt_lug_en_14_3_23'])\n\nif config['back_translation_training_data']:\n    config['training_subset_ids'].extend(['backtranslated-from-eng-google', 'backtranslated-from-lug-google' ])\n\nconfig['training_subset_ids'] = config['training_subset_ids']*5\n\nif config['mt560_training_data']:\n    config['training_subset_ids'].extend([\n        'mt560_ach', 'mt560_lug', 'mt560_nyn','mt560_luo'])\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T11:33:17.830786Z","iopub.execute_input":"2023-03-17T11:33:17.831187Z","iopub.status.idle":"2023-03-17T11:33:17.839108Z","shell.execute_reply.started":"2023-03-17T11:33:17.831143Z","shell.execute_reply":"2023-03-17T11:33:17.837973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.exists('salt-translation-plus-external-datasets-15-3-23'):\n    !wget https://sunbird-translate.s3.us-east-2.amazonaws.com/salt-translation-plus-external-datasets-15-3-23.zip\n    !unzip salt-translation-plus-external-datasets-15-3-23.zip\n    display.clear_output()","metadata":{"execution":{"iopub.status.busy":"2023-03-17T11:33:17.840675Z","iopub.execute_input":"2023-03-17T11:33:17.841354Z","iopub.status.idle":"2023-03-17T11:33:17.848902Z","shell.execute_reply.started":"2023-03-17T11:33:17.841305Z","shell.execute_reply":"2023-03-17T11:33:17.847795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\ntraining_subsets = [\n    salt.translation_dataset(\n        path=f'{config[\"data_dir\"]}/{id}.jsonl',\n        source_language=config['source_language'],\n        target_language=config['target_language'],\n        allow_target_language_in_source=False,\n        prefix_target_language_in_source=False,\n        languages_to_include=config['eval_languages'],\n        keep_unaugmented_src = False)\n    for id in tqdm(config['training_subset_ids'])\n]","metadata":{"execution":{"iopub.status.busy":"2023-03-17T11:33:17.851948Z","iopub.execute_input":"2023-03-17T11:33:17.852395Z","iopub.status.idle":"2023-03-17T11:33:31.250947Z","shell.execute_reply.started":"2023-03-17T11:33:17.852360Z","shell.execute_reply":"2023-03-17T11:33:31.249816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_probabilities = np.array([len(s) for s in training_subsets])\n# sample_probabilities = sample_probabilities * np.array(\n#     [config['mt560_relative_sample_rate'] if ('mt560' in id) or ('google' in id)  else 1.0\n#      for id in config['training_subset_ids']])\n# sample_probabilities = sample_probabilities / np.sum(sample_probabilities)\n\n# train_data_raw = datasets.interleave_datasets(\n#     training_subsets, sample_probabilities)\ntrain_data_raw = datasets.concatenate_datasets(training_subsets)\ntrain_data_raw = train_data_raw.shuffle()\ntrain_data_raw = train_data_raw.flatten_indices()","metadata":{"execution":{"iopub.status.busy":"2023-03-17T11:33:31.252618Z","iopub.execute_input":"2023-03-17T11:33:31.254696Z","iopub.status.idle":"2023-03-17T11:33:58.863625Z","shell.execute_reply.started":"2023-03-17T11:33:31.254651Z","shell.execute_reply":"2023-03-17T11:33:58.862689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_subsets = [\n    salt.translation_dataset(\n        path=f'{config[\"data_dir\"]}/salt-dev.jsonl',\n        source_language=language,\n        target_language=\"eng\",\n        keep_unaugmented_src = False,\n        allow_target_language_in_source=False,\n        prefix_target_language_in_source=False\n    )\n    for language in config['eval_languages']\n]\nvalidation_data_raw = datasets.concatenate_datasets(validation_subsets)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T11:33:58.866065Z","iopub.execute_input":"2023-03-17T11:33:58.866468Z","iopub.status.idle":"2023-03-17T11:33:58.908922Z","shell.execute_reply.started":"2023-03-17T11:33:58.866428Z","shell.execute_reply":"2023-03-17T11:33:58.908038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentence_format(input):\n    '''Ensure capital letter at the start and full stop at the end.'''\n    try:\n        input = input[0].capitalize() + input[1:]\n        if input[-1] not in ['.', '!', '?']:\n            input = input + '.'\n    except:\n        return \"\"\n    return input\n\ndef preprocess(examples):\n    normalizer = sacremoses.MosesPunctNormalizer()  \n    inputs = []\n    targets = []\n    for input, target in zip(examples['source'], examples['target']):\n        if not len(input):\n          input = target\n        inputs.append(sentence_format(normalizer.normalize(input)))\n        targets.append(sentence_format(normalizer.normalize(target)))\n    \n    model_inputs = tokenizer(\n        inputs, text_target=targets,\n        max_length=config['max_input_length'], truncation=True)\n\n    return model_inputs\n\ndef postprocess(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\n\ndef compute_metrics(eval_preds, eval_languages, samples_per_language):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n        \n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds, decoded_labels = postprocess(decoded_preds, decoded_labels)\n    result = {}\n    for i, lang in enumerate(eval_languages):\n        \n        result_subset = metric.compute(\n            predictions=decoded_preds[\n                i*samples_per_language:(i+1)*samples_per_language],\n            references=decoded_labels[\n                i*samples_per_language:(i+1)*samples_per_language])\n        result[f\"BLEU_{lang}\"] = result_subset[\"score\"]\n        \n    result[\"BLEU_mean\"] = np.mean(\n        [result[f\"BLEU_{lang}\"] for lang in eval_languages])\n    \n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-03-17T11:33:58.912586Z","iopub.execute_input":"2023-03-17T11:33:58.912852Z","iopub.status.idle":"2023-03-17T11:33:58.924884Z","shell.execute_reply.started":"2023-03-17T11:33:58.912826Z","shell.execute_reply":"2023-03-17T11:33:58.923729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = transformers.AutoModelForSeq2SeqLM.from_pretrained(config['model_checkpoint'])\ntokenizer = transformers.AutoTokenizer.from_pretrained(config['model_checkpoint'])\ndata_collator = transformers.DataCollatorForSeq2Seq(tokenizer, model = model) \nmetric = datasets.load_metric('sacrebleu')","metadata":{"execution":{"iopub.status.busy":"2023-03-17T11:33:58.926608Z","iopub.execute_input":"2023-03-17T11:33:58.926996Z","iopub.status.idle":"2023-03-17T11:34:06.137711Z","shell.execute_reply.started":"2023-03-17T11:33:58.926952Z","shell.execute_reply":"2023-03-17T11:34:06.136688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if config['target_language'] == 'many':\n    replacements = {'nyn': 'kin',\n                    'lgg': 'lin',\n                    'ach': 'tso',\n                    'teo': 'som',\n                    'luo': 'sna'}\n    for r in replacements:\n        if (f'>>{r}<<' not in tokenizer.encoder and\n            f'{replacements[r]}' in tokenizer.encoder):\n            tokenizer.encoder[f\">>{r}<<\"] = tokenizer.encoder[f\"{replacements[r]}\"]\n            del tokenizer.encoder[f\"{replacements[r]}\"]\n\n    # Check that all the evaluation language codes are mapped to something.\n    for r in config['eval_languages']:\n        if f'>>{r}<<' not in tokenizer.encoder:\n            raise ValueError(f'Language code {r} not found in the encoder.')","metadata":{"execution":{"iopub.status.busy":"2023-03-17T11:34:06.139138Z","iopub.execute_input":"2023-03-17T11:34:06.139580Z","iopub.status.idle":"2023-03-17T11:34:06.146827Z","shell.execute_reply.started":"2023-03-17T11:34:06.139539Z","shell.execute_reply":"2023-03-17T11:34:06.145770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data  = train_data_raw.map(\n    preprocess,\n    remove_columns=['source', 'target', 'source_language', 'target_language'],\n    batched=True)\n\nvalidation_data  = validation_data_raw.map(\n    preprocess,\n    remove_columns=['source', 'target', 'source_language', 'target_language'],\n    batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T11:34:06.148400Z","iopub.execute_input":"2023-03-17T11:34:06.149065Z","iopub.status.idle":"2023-03-17T11:48:03.809241Z","shell.execute_reply.started":"2023-03-17T11:34:06.149026Z","shell.execute_reply":"2023-03-17T11:48:03.807768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb_key\")\nimport os\nos.environ[\"WANDB_API_KEY\"] = secret_value_0\nwandb.init(project=config['wandb_project'], config=config, entity=\"azawahry\")\n\ntransformers.logging.set_verbosity_error()\n\ntrainer = transformers.Seq2SeqTrainer(\n    model,\n    config['train_settings'],\n    train_dataset = train_data,\n    eval_dataset = validation_data,\n    data_collator = data_collator,\n    tokenizer = tokenizer,\n    compute_metrics = lambda x: compute_metrics(\n        x, config['eval_languages'][:-1], config['validation_samples_per_language']),\n    callbacks = [\n        transformers.EarlyStoppingCallback(\n            early_stopping_patience = config['early_stopping_patience'])],\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T11:52:01.033779Z","iopub.execute_input":"2023-03-17T11:52:01.034275Z","iopub.status.idle":"2023-03-17T11:52:38.190474Z","shell.execute_reply.started":"2023-03-17T11:52:01.034227Z","shell.execute_reply":"2023-03-17T11:52:38.189453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if config['eval_pretrained_model']:\n#     trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2023-03-17T11:52:38.195292Z","iopub.execute_input":"2023-03-17T11:52:38.197742Z","iopub.status.idle":"2023-03-17T11:52:38.204177Z","shell.execute_reply.started":"2023-03-17T11:52:38.197700Z","shell.execute_reply":"2023-03-17T11:52:38.203008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-03-17T11:52:38.209112Z","iopub.execute_input":"2023-03-17T11:52:38.210546Z","iopub.status.idle":"2023-03-17T15:36:51.814471Z","shell.execute_reply.started":"2023-03-17T11:52:38.210508Z","shell.execute_reply":"2023-03-17T15:36:51.813396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}